{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Source: UCI Machine Learning Repository\n",
    "- Th√®me: Pr√©diction d'abandon scolaire et r√©ussite acad√©mique\n",
    "- √âtablissement: Enseignement sup√©rieur au Portugal\n",
    "- URL: https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"../data/dataset.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Taille du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre d'observations: {df.shape[0]:,}\")\n",
    "print(f\"Nombre de variables: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Liste des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Typage des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description du dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Distribution de la variable cible (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = df[\"Target\"].value_counts()\n",
    "target_pct = df[\"Target\"].value_counts(normalize=True) * 100\n",
    "\n",
    "for category in target_counts.index:\n",
    "    count = target_counts[category]\n",
    "    pct = target_pct[category]\n",
    "    print(f\"- {category}: {count:,} √©tudiants ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"Set2\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Cr√©ation du pie chart\n",
    "# target_counts = df['Target'].value_counts()\n",
    "colors = [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"]  # Couleurs harmonieuses\n",
    "explode = (0.05, 0.05, 0.05)  # L√©g√®re s√©paration des parts\n",
    "\n",
    "plt.pie(\n",
    "    target_counts.values,\n",
    "    labels=target_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    "    textprops={\"fontsize\": 12, \"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution de la variable cible (Target)\", fontsize=14, weight=\"bold\", pad=20)\n",
    "\n",
    "plt.axis(\"equal\")  # Pour avoir un cercle parfait\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Premieres lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse binaire: Dropout vs Non-Dropout\n",
    "print(\"üéØ Classification Binaire: Dropout vs Non-Dropout\\n\")\n",
    "\n",
    "# Cr√©er une nouvelle variable binaire\n",
    "df[\"Dropout_Binary\"] = df[\"Target\"].apply(lambda x: \"Dropout\" if x == \"Dropout\" else \"Non-Dropout\")\n",
    "\n",
    "# Calculer les proportions\n",
    "binary_counts = df[\"Dropout_Binary\"].value_counts()\n",
    "binary_pct = df[\"Dropout_Binary\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Distribution:\")\n",
    "for category in binary_counts.index:\n",
    "    count = binary_counts[category]\n",
    "    pct = binary_pct[category]\n",
    "    print(f\"  ‚Ä¢ {category}: {count:,} √©tudiants ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal: {binary_counts.sum():,} √©tudiants\")\n",
    "\n",
    "# D√©tail de la composition de Non-Dropout\n",
    "print(\"\\nüìã Composition de 'Non-Dropout':\")\n",
    "non_dropout_detail = df[df[\"Dropout_Binary\"] == \"Non-Dropout\"][\"Target\"].value_counts()\n",
    "for category in non_dropout_detail.index:\n",
    "    count = non_dropout_detail[category]\n",
    "    pct = (count / binary_counts[\"Non-Dropout\"]) * 100\n",
    "    print(f\"  ‚Ä¢ {category}: {count:,} ({pct:.1f}% des Non-Dropout)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation: Dropout vs Non-Dropout avec pie chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Graphique 1: Distribution binaire\n",
    "colors_binary = [\"#e74c3c\", \"#2ecc71\"]  # Rouge pour Dropout, Vert pour Non-Dropout\n",
    "explode_binary = (0.1, 0)  # Faire ressortir Dropout\n",
    "\n",
    "axes[0].pie(\n",
    "    binary_counts.values,\n",
    "    labels=binary_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors_binary,\n",
    "    explode=explode_binary,\n",
    "    shadow=True,\n",
    "    textprops={\"fontsize\": 12, \"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "axes[0].set_title(\"Classification Binaire\\nDropout vs Non-Dropout\", fontsize=14, weight=\"bold\")\n",
    "\n",
    "# Graphique 2: Distribution originale (rappel)\n",
    "colors_original = [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"]\n",
    "target_counts = df[\"Target\"].value_counts()\n",
    "explode_original = (0.05, 0.05, 0.05)\n",
    "\n",
    "axes[1].pie(\n",
    "    target_counts.values,\n",
    "    labels=target_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors_original,\n",
    "    explode=explode_original,\n",
    "    shadow=True,\n",
    "    textprops={\"fontsize\": 12, \"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "axes[1].set_title(\"Classification Multi-classe\\n(Original)\", fontsize=14, weight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Comparaison:\")\n",
    "print(\n",
    "    f\"   Approche binaire: {binary_pct['Dropout']:.1f}% Dropout vs {binary_pct['Non-Dropout']:.1f}% Non-Dropout\"\n",
    ")\n",
    "print(f\"   Ratio de d√©s√©quilibre: 1:{binary_counts['Non-Dropout'] / binary_counts['Dropout']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Phase 2 : Qualit√© des Donn√©es\n",
    "\n",
    "Avant d'analyser les relations entre variables, v√©rifions la qualit√© de nos donn√©es :\n",
    "1. **Valeurs manquantes** - Y a-t-il des donn√©es absentes ?\n",
    "2. **Doublons** - Des lignes sont-elles dupliqu√©es ?\n",
    "3. **Outliers** - Des valeurs aberrantes existent-elles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 2.1 Analyse des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\"Manquantes\": missing, \"Pourcentage (%)\": missing_pct.round(2)})\n",
    "missing_df = missing_df[missing_df[\"Manquantes\"] > 0].sort_values(\"Manquantes\", ascending=False)\n",
    "\n",
    "if not len(missing_df):\n",
    "    print(\"‚úÖ Aucune valeur manquante dans le dataset !\")\n",
    "    print(f\"- Toutes les {df.shape[1]} colonnes sont compl√®tes\")\n",
    "    print(f\"- {df.shape[0]:,} lignes sans donn√©es manquantes\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Valeurs manquantes d√©tect√©es:\\n\")\n",
    "    display(missing_df)\n",
    "\n",
    "# R√©sum√© rapide\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_ratio = total_missing / total_cells\n",
    "print(\n",
    "    f\"R√©sum√©: {total_missing:,} valeurs manquantes sur {total_cells:,} cellules ({total_ratio:.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 2.2 Analyse des valeurs dupliqu√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doublons complets (toutes les colonnes identiques)\n",
    "duplicates_full = df.duplicated().sum()\n",
    "print(\n",
    "    f\"üîç Lignes compl√®tement dupliqu√©es: {duplicates_full} ({(duplicates_full / len(df)) * 100:.2f}%)\"\n",
    ")\n",
    "\n",
    "if duplicates_full > 0:\n",
    "    print(\"Exemples de doublons:\")\n",
    "    display(df[df.duplicated(keep=False)].head(10))\n",
    "else:\n",
    "    print(\"- Aucun doublon complet d√©tect√© ‚úÖ\")\n",
    "\n",
    "# V√©rifier aussi les doublons partiels (sans la colonne Target)\n",
    "cols_without_target = [col for col in df.columns if col not in [\"Target\", \"Dropout_Binary\"]]\n",
    "duplicates_partial = df.duplicated(subset=cols_without_target).sum()\n",
    "print(f\"\\nüîç Doublons partiels (m√™mes features, Target diff√©rent): {duplicates_partial}\")\n",
    "\n",
    "if duplicates_partial:\n",
    "    print(\"- Des √©tudiants avec les m√™mes caract√©ristiques ont des r√©sultats diff√©rents\")\n",
    "    print(\"- Cela peut indiquer de la variabilit√© naturelle ou des erreurs de saisie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## D√©tection des outliers - Variables num√©riques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palette de couleurs pour √©viter les warnings\n",
    "colors = {\"Dropout\": \"#e74c3c\", \"Non-Dropout\": \"#2ecc71\"}\n",
    "order = [\"Dropout\", \"Non-Dropout\"]\n",
    "\n",
    "# S√©lection des variables num√©riques principales\n",
    "numeric_cols = [\n",
    "    \"Age at enrollment\",\n",
    "    \"Admission grade\",\n",
    "    \"Previous qualification (grade)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "]\n",
    "\n",
    "# Cr√©ation des boxplots avec seaborn (√©vite les warnings)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"Dropout_Binary\",\n",
    "        y=col,\n",
    "        hue=\"Dropout_Binary\",\n",
    "        hue_order=order,\n",
    "        palette=colors,\n",
    "        legend=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(col, fontsize=11, weight=\"bold\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Valeur\")\n",
    "\n",
    "# Supprimer le dernier subplot vide\n",
    "axes[-1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Boxplots des Variables Num√©riques Cl√©s\\n(Comparaison Dropout vs Non-Dropout)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation des boxplots:\")\n",
    "print(\"- Les points au-del√† des moustaches sont des outliers potentiels\")\n",
    "print(\"- Comparez les m√©dianes (ligne horizontale) entre Dropout et Non-Dropout\")\n",
    "print(\"- Des diff√©rences marqu√©es sugg√®rent un pouvoir pr√©dictif de la variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3b Quantification des outliers avec la m√©thode IQR\n",
    "# ‚ö†Ô∏è UNIQUEMENT sur les variables CONTINUES (pas les cat√©gorielles encod√©es)\n",
    "print(\"üìä Quantification des Outliers (M√©thode IQR)\\n\" + \"=\" * 50)\n",
    "\n",
    "\n",
    "def count_outliers_iqr(df, column):\n",
    "    \"\"\"Compte les outliers selon la m√©thode IQR\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "\n",
    "# Variables CONTINUES uniquement (exclure les cat√©gorielles encod√©es en num√©rique)\n",
    "continuous_cols = [\n",
    "    \"Age at enrollment\",\n",
    "    \"Admission grade\",\n",
    "    \"Previous qualification (grade)\",\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\n",
    "    \"Curricular units 2nd sem (credited)\",\n",
    "    \"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\n",
    "    \"Unemployment rate\",\n",
    "    \"Inflation rate\",\n",
    "    \"GDP\",\n",
    "]\n",
    "\n",
    "# Variables cat√©gorielles exclues (m√™me si encod√©es en num√©rique)\n",
    "categorical_excluded = [\n",
    "    \"Marital status\",\n",
    "    \"Application mode\",\n",
    "    \"Application order\",\n",
    "    \"Course\",\n",
    "    \"Daytime/evening attendance\",\n",
    "    \"Previous qualification\",\n",
    "    \"Nacionality\",\n",
    "    \"Mother's qualification\",\n",
    "    \"Father's qualification\",\n",
    "    \"Mother's occupation\",\n",
    "    \"Father's occupation\",\n",
    "    \"Displaced\",\n",
    "    \"Educational special needs\",\n",
    "    \"Debtor\",\n",
    "    \"Tuition fees up to date\",\n",
    "    \"Gender\",\n",
    "    \"Scholarship holder\",\n",
    "    \"International\",\n",
    "]\n",
    "\n",
    "print(f\"üìã Analyse de {len(continuous_cols)} variables continues\")\n",
    "print(f\"   (Variables cat√©gorielles exclues: {len(categorical_excluded)})\\n\")\n",
    "\n",
    "outlier_summary = []\n",
    "for col in continuous_cols:\n",
    "    if col in df.columns:\n",
    "        count, lower, upper = count_outliers_iqr(df, col)\n",
    "        pct = (count / len(df)) * 100\n",
    "        if count > 0:\n",
    "            outlier_summary.append(\n",
    "                {\n",
    "                    \"Variable\": col,\n",
    "                    \"Outliers\": count,\n",
    "                    \"Pourcentage\": f\"{pct:.2f}%\",\n",
    "                    \"Borne inf\": f\"{lower:.2f}\",\n",
    "                    \"Borne sup\": f\"{upper:.2f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df.sort_values(\"Outliers\", ascending=False)\n",
    "\n",
    "if len(outlier_df) > 0:\n",
    "    print(\"‚ö†Ô∏è Variables continues avec outliers d√©tect√©s:\\n\")\n",
    "    display(outlier_df)\n",
    "    print(\n",
    "        \"\\nüí° Note: Ces outliers peuvent √™tre l√©gitimes (ex: √©tudiants plus √¢g√©s, excellentes notes)\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ Aucun outlier significatif d√©tect√© dans les variables continues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Phase 3 : Analyse Bivari√©e - Variables Num√©riques vs Dropout\n",
    "\n",
    "**Objectif** : Identifier quelles variables num√©riques diff√©rencient les √©tudiants qui abandonnent de ceux qui r√©ussissent.\n",
    "\n",
    "**Questions cl√©s** :\n",
    "- Les Dropout ont-ils des notes d'admission plus faibles ?\n",
    "- L'√¢ge est-il un facteur de risque ?\n",
    "- La performance au 1er semestre pr√©dit-elle l'abandon ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1a Notes d'admission et qualification ant√©rieure vs Dropout\n",
    "print(\"üìä Notes d'Admission vs Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Palette de couleurs pour √©viter les warnings\n",
    "colors = {\"Dropout\": \"#e74c3c\", \"Non-Dropout\": \"#2ecc71\"}\n",
    "order = [\"Dropout\", \"Non-Dropout\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Admission grade\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Dropout_Binary\",\n",
    "    y=\"Admission grade\",\n",
    "    hue=\"Dropout_Binary\",\n",
    "    hue_order=order,\n",
    "    palette=colors,\n",
    "    legend=False,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Note d'admission par groupe\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Note d'admission (0-200)\")\n",
    "\n",
    "# Previous qualification grade\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Dropout_Binary\",\n",
    "    y=\"Previous qualification (grade)\",\n",
    "    hue=\"Dropout_Binary\",\n",
    "    hue_order=order,\n",
    "    palette=colors,\n",
    "    legend=False,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"Note de qualification ant√©rieure par groupe\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Note qualification (0-200)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques comparatives\n",
    "print(\"\\nüìà Statistiques comparatives:\")\n",
    "for col in [\"Admission grade\", \"Previous qualification (grade)\"]:\n",
    "    dropout_mean = df[df[\"Dropout_Binary\"] == \"Dropout\"][col].mean()\n",
    "    non_dropout_mean = df[df[\"Dropout_Binary\"] == \"Non-Dropout\"][col].mean()\n",
    "    diff = non_dropout_mean - dropout_mean\n",
    "    print(f\"\\n   {col}:\")\n",
    "    print(f\"- Dropout:     {dropout_mean:.1f}\")\n",
    "    print(f\"- Non-Dropout: {non_dropout_mean:.1f}\")\n",
    "    print(\n",
    "        f\"- Diff√©rence:  {diff:+.1f} points ({'‚úÖ Significatif' if abs(diff) > 5 else '‚ö™ Faible'})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1b √Çge √† l'inscription vs Dropout\n",
    "print(\"üìä √Çge √† l'inscription vs Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Palette de couleurs pour √©viter les warnings\n",
    "colors = {\"Dropout\": \"#e74c3c\", \"Non-Dropout\": \"#2ecc71\"}\n",
    "order = [\"Dropout\", \"Non-Dropout\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Dropout_Binary\",\n",
    "    y=\"Age at enrollment\",\n",
    "    hue=\"Dropout_Binary\",\n",
    "    hue_order=order,\n",
    "    palette=colors,\n",
    "    legend=False,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Distribution de l'√¢ge par groupe\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"√Çge √† l'inscription\")\n",
    "\n",
    "# Histogramme superpos√©\n",
    "for label, color in [(\"Dropout\", \"#e74c3c\"), (\"Non-Dropout\", \"#2ecc71\")]:\n",
    "    subset = df[df[\"Dropout_Binary\"] == label][\"Age at enrollment\"]\n",
    "    axes[1].hist(subset, bins=30, alpha=0.6, label=label, color=color, density=True)\n",
    "\n",
    "axes[1].set_title(\"Distribution de l'√¢ge (densit√©)\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_xlabel(\"√Çge √† l'inscription\")\n",
    "axes[1].set_ylabel(\"Densit√©\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques comparatives\n",
    "print(\"\\nüìà Statistiques sur l'√¢ge:\")\n",
    "dropout_age = df[df[\"Dropout_Binary\"] == \"Dropout\"][\"Age at enrollment\"]\n",
    "non_dropout_age = df[df[\"Dropout_Binary\"] == \"Non-Dropout\"][\"Age at enrollment\"]\n",
    "\n",
    "print(\"\\n   Dropout:\")\n",
    "print(f\"- Moyenne: {dropout_age.mean():.1f} ans | M√©diane: {dropout_age.median():.0f} ans\")\n",
    "print(f\"- Min: {dropout_age.min():.0f} | Max: {dropout_age.max():.0f}\")\n",
    "\n",
    "print(\"\\n   Non-Dropout:\")\n",
    "print(f\"- Moyenne: {non_dropout_age.mean():.1f} ans | M√©diane: {non_dropout_age.median():.0f} ans\")\n",
    "print(f\"- Min: {non_dropout_age.min():.0f} | Max: {non_dropout_age.max():.0f}\")\n",
    "\n",
    "diff = dropout_age.mean() - non_dropout_age.mean()\n",
    "print(\n",
    "    f\"\\n   üí° Les Dropout sont en moyenne {abs(diff):.1f} ans {'plus √¢g√©s' if diff > 0 else 'plus jeunes'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1c Performance 1er Semestre vs Dropout\n",
    "print(\"üìä Performance 1er Semestre vs Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Palette de couleurs pour √©viter les warnings\n",
    "colors = {\"Dropout\": \"#e74c3c\", \"Non-Dropout\": \"#2ecc71\"}\n",
    "order = [\"Dropout\", \"Non-Dropout\"]\n",
    "\n",
    "sem1_cols = [\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, col in enumerate(sem1_cols):\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"Dropout_Binary\",\n",
    "        y=col,\n",
    "        hue=\"Dropout_Binary\",\n",
    "        hue_order=order,\n",
    "        palette=colors,\n",
    "        legend=False,\n",
    "        ax=axes[i],\n",
    "    )\n",
    "    # Titre simplifi√©\n",
    "    short_name = col.replace(\"Curricular units 1st sem \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    axes[i].set_title(f\"1er Sem: {short_name.capitalize()}\", fontsize=11, weight=\"bold\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Variables du 1er Semestre - Comparaison Dropout vs Non-Dropout\",\n",
    "    fontsize=13,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques cl√©s\n",
    "print(\"\\nüìà Statistiques du 1er semestre:\")\n",
    "key_col = \"Curricular units 1st sem (approved)\"\n",
    "dropout_val = df[df[\"Dropout_Binary\"] == \"Dropout\"][key_col].mean()\n",
    "non_dropout_val = df[df[\"Dropout_Binary\"] == \"Non-Dropout\"][key_col].mean()\n",
    "\n",
    "print(\"\\n   Unit√©s valid√©es au 1er semestre:\")\n",
    "print(f\"- Dropout:     {dropout_val:.2f} unit√©s en moyenne\")\n",
    "print(f\"- Non-Dropout: {non_dropout_val:.2f} unit√©s en moyenne\")\n",
    "print(\n",
    "    f\"- Ratio:       {non_dropout_val / max(dropout_val, 0.01):.1f}x plus d'unit√©s valid√©es pour Non-Dropout\"\n",
    ")\n",
    "\n",
    "key_col2 = \"Curricular units 1st sem (grade)\"\n",
    "dropout_grade = df[df[\"Dropout_Binary\"] == \"Dropout\"][key_col2].mean()\n",
    "non_dropout_grade = df[df[\"Dropout_Binary\"] == \"Non-Dropout\"][key_col2].mean()\n",
    "print(\"\\n   Note moyenne au 1er semestre:\")\n",
    "print(f\"- Dropout:     {dropout_grade:.2f}/20\")\n",
    "print(f\"- Non-Dropout: {non_dropout_grade:.2f}/20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1d Performance 2√®me Semestre vs Dropout\n",
    "print(\"üìä Performance 2√®me Semestre vs Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Palette de couleurs pour √©viter les warnings\n",
    "colors = {\"Dropout\": \"#e74c3c\", \"Non-Dropout\": \"#2ecc71\"}\n",
    "order = [\"Dropout\", \"Non-Dropout\"]\n",
    "\n",
    "sem2_cols = [\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "    \"Curricular units 2nd sem (enrolled)\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, col in enumerate(sem2_cols):\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"Dropout_Binary\",\n",
    "        y=col,\n",
    "        hue=\"Dropout_Binary\",\n",
    "        hue_order=order,\n",
    "        palette=colors,\n",
    "        legend=False,\n",
    "        ax=axes[i],\n",
    "    )\n",
    "    short_name = col.replace(\"Curricular units 2nd sem \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    axes[i].set_title(f\"2√®me Sem: {short_name.capitalize()}\", fontsize=11, weight=\"bold\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Variables du 2√®me Semestre - Comparaison Dropout vs Non-Dropout\",\n",
    "    fontsize=13,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparaison 1er vs 2√®me semestre\n",
    "print(\"\\nüìà Comparaison √©volution entre semestres:\")\n",
    "print(\"\\n   | Groupe       | Sem 1 (approved) | Sem 2 (approved) | √âvolution |\")\n",
    "print(\"   |--------------|------------------|------------------|-----------|\")\n",
    "\n",
    "for group in [\"Dropout\", \"Non-Dropout\"]:\n",
    "    sem1_mean = df[df[\"Dropout_Binary\"] == group][\"Curricular units 1st sem (approved)\"].mean()\n",
    "    sem2_mean = df[df[\"Dropout_Binary\"] == group][\"Curricular units 2nd sem (approved)\"].mean()\n",
    "    evolution = sem2_mean - sem1_mean\n",
    "    print(f\"   | {group:12} | {sem1_mean:16.2f} | {sem2_mean:16.2f} | {evolution:+9.2f} |\")\n",
    "\n",
    "print(\"\\nüí° Observation: Comment √©volue la performance entre les deux semestres ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Phase 3.2 : Variables Cat√©gorielles vs Dropout\n",
    "\n",
    "**Objectif** : Identifier quelles cat√©gories sont associ√©es √† un risque plus √©lev√© de Dropout.\n",
    "\n",
    "**Questions cl√©s** :\n",
    "- Certains programmes ont-ils plus d'abandons ?\n",
    "- Les boursiers abandonnent-ils moins ?\n",
    "- Le genre, l'√©tat civil ou les cours du soir influencent-ils le Dropout ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2a Analyse des variables binaires vs Dropout\n",
    "print(\"üìä Variables Binaires vs Taux de Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Variables binaires √† analyser (sans Daytime/evening attendance qui a un nom probl√©matique)\n",
    "binary_vars = [\n",
    "    (\"Gender\", {0: \"Femme\", 1: \"Homme\"}),\n",
    "    (\"Scholarship holder\", {0: \"Non boursier\", 1: \"Boursier\"}),\n",
    "    (\"Debtor\", {0: \"Non d√©biteur\", 1: \"D√©biteur\"}),\n",
    "    (\"Tuition fees up to date\", {0: \"Non √† jour\", 1: \"√Ä jour\"}),\n",
    "    (\"Displaced\", {0: \"Local\", 1: \"D√©plac√©\"}),\n",
    "    (\"International\", {0: \"National\", 1: \"International\"}),\n",
    "]\n",
    "\n",
    "# Calculer le taux de dropout pour chaque variable\n",
    "results = []\n",
    "for var, labels in binary_vars:\n",
    "    for val, label in labels.items():\n",
    "        subset = df[df[var] == val]\n",
    "        total = len(subset)\n",
    "        dropout_count = len(subset[subset[\"Dropout_Binary\"] == \"Dropout\"])\n",
    "        dropout_rate = (dropout_count / total * 100) if total > 0 else 0\n",
    "        results.append(\n",
    "            {\n",
    "                \"Variable\": var,\n",
    "                \"Cat√©gorie\": label,\n",
    "                \"Total\": total,\n",
    "                \"Dropout\": dropout_count,\n",
    "                \"Taux Dropout (%)\": round(dropout_rate, 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Afficher le tableau\n",
    "print(\"\\nüìã Taux de Dropout par variable binaire:\\n\")\n",
    "display(results_df)\n",
    "\n",
    "# Identifier les facteurs de risque\n",
    "print(\"\\nüî¥ Facteurs augmentant le risque de Dropout:\")\n",
    "for var, labels in binary_vars:\n",
    "    rates = results_df[results_df[\"Variable\"] == var][\"Taux Dropout (%)\"].values\n",
    "    if len(rates) == 2 and abs(rates[0] - rates[1]) > 5:\n",
    "        higher = labels[0] if rates[0] > rates[1] else labels[1]\n",
    "        diff = abs(rates[0] - rates[1])\n",
    "        print(f\"- {var}: '{higher}' ‚Üí +{diff:.1f}% de dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2b Visualisation des variables binaires\n",
    "print(\"üìä Visualisation du Taux de Dropout par Variable Binaire\\n\" + \"=\" * 50)\n",
    "\n",
    "# Variables binaires (sans Daytime/evening attendance)\n",
    "binary_vars_simple = [\n",
    "    \"Gender\",\n",
    "    \"Scholarship holder\",\n",
    "    \"Debtor\",\n",
    "    \"Tuition fees up to date\",\n",
    "    \"Displaced\",\n",
    "    \"International\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(binary_vars_simple):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Calculer les pourcentages\n",
    "    cross_tab = pd.crosstab(df[var], df[\"Dropout_Binary\"], normalize=\"index\") * 100\n",
    "\n",
    "    # Plot\n",
    "    cross_tab.plot(kind=\"bar\", ax=ax, color=[\"#2ecc71\", \"#e74c3c\"], edgecolor=\"black\")\n",
    "    ax.set_title(f\"{var}\", fontsize=11, weight=\"bold\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Pourcentage (%)\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "    ax.legend([\"Non-Dropout\", \"Dropout\"], loc=\"upper right\")\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "    # Ajouter la ligne de r√©f√©rence (taux global)\n",
    "    global_dropout_rate = (df[\"Dropout_Binary\"] == \"Dropout\").mean() * 100\n",
    "    ax.axhline(\n",
    "        y=global_dropout_rate,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.5,\n",
    "        label=\"Taux global\",\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Taux de Dropout vs Non-Dropout par Variable Binaire\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Taux de dropout global: {global_dropout_rate:.1f}%\")\n",
    "print(\"   (Ligne rouge pointill√©e = r√©f√©rence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2c Programme d'√©tudes (Course) vs Dropout\n",
    "print(\"üìä Programme d'√âtudes vs Taux de Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Mapping des codes de programme\n",
    "course_mapping = {\n",
    "    33: \"Biofuel Production\",\n",
    "    171: \"Animation & Multimedia\",\n",
    "    8014: \"Social Service (soir)\",\n",
    "    9003: \"Agronomy\",\n",
    "    9070: \"Communication Design\",\n",
    "    9085: \"Veterinary Nursing\",\n",
    "    9119: \"Informatics Engineering\",\n",
    "    9130: \"Equinculture\",\n",
    "    9147: \"Management\",\n",
    "    9238: \"Social Service\",\n",
    "    9254: \"Tourism\",\n",
    "    9500: \"Nursing\",\n",
    "    9556: \"Oral Hygiene\",\n",
    "    9670: \"Advertising & Marketing\",\n",
    "    9773: \"Journalism & Communication\",\n",
    "    9853: \"Basic Education\",\n",
    "    9991: \"Management (soir)\",\n",
    "}\n",
    "\n",
    "# Calculer le taux de dropout par programme\n",
    "course_stats = (\n",
    "    df.groupby(\"Course\")\n",
    "    .agg({\"Dropout_Binary\": lambda x: (x == \"Dropout\").sum(), \"Target\": \"count\"})\n",
    "    .rename(columns={\"Dropout_Binary\": \"Dropouts\", \"Target\": \"Total\"})\n",
    ")\n",
    "\n",
    "course_stats[\"Taux Dropout (%)\"] = (course_stats[\"Dropouts\"] / course_stats[\"Total\"] * 100).round(1)\n",
    "course_stats[\"Programme\"] = course_stats.index.map(course_mapping)\n",
    "course_stats = course_stats.sort_values(\"Taux Dropout (%)\", ascending=False)\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors = [\n",
    "    \"#e74c3c\" if x > 40 else \"#f39c12\" if x > 30 else \"#2ecc71\"\n",
    "    for x in course_stats[\"Taux Dropout (%)\"]\n",
    "]\n",
    "\n",
    "bars = ax.barh(\n",
    "    course_stats[\"Programme\"],\n",
    "    course_stats[\"Taux Dropout (%)\"],\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax.set_xlabel(\"Taux de Dropout (%)\", fontsize=12)\n",
    "ax.set_title(\"Taux de Dropout par Programme d'√âtudes\", fontsize=14, weight=\"bold\")\n",
    "ax.axvline(\n",
    "    x=global_dropout_rate,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Moyenne ({global_dropout_rate:.1f}%)\",\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, val in zip(bars, course_stats[\"Taux Dropout (%)\"]):\n",
    "    ax.text(\n",
    "        bar.get_width() + 0.5,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{val}%\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R√©sum√©\n",
    "print(\"\\nüî¥ Programmes √† HAUT risque (>40% dropout):\")\n",
    "high_risk = course_stats[course_stats[\"Taux Dropout (%)\"] > 40]\n",
    "for _, row in high_risk.iterrows():\n",
    "    print(\n",
    "        f\"- {row['Programme']}: {row['Taux Dropout (%)']}% ({row['Dropouts']}/{row['Total']} √©tudiants)\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüü¢ Programmes √† FAIBLE risque (<25% dropout):\")\n",
    "low_risk = course_stats[course_stats[\"Taux Dropout (%)\"] < 25]\n",
    "for _, row in low_risk.iterrows():\n",
    "    print(\n",
    "        f\"- {row['Programme']}: {row['Taux Dropout (%)']}% ({row['Dropouts']}/{row['Total']} √©tudiants)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2d Statut Matrimonial vs Dropout\n",
    "print(\"üìä Statut Matrimonial vs Taux de Dropout\\n\" + \"=\" * 50)\n",
    "\n",
    "# Mapping des codes d'√©tat civil\n",
    "marital_mapping = {\n",
    "    1: \"C√©libataire\",\n",
    "    2: \"Mari√©(e)\",\n",
    "    3: \"Veuf/Veuve\",\n",
    "    4: \"Divorc√©(e)\",\n",
    "    5: \"Union de fait\",\n",
    "    6: \"S√©par√©(e)\",\n",
    "}\n",
    "\n",
    "# Calculer le taux de dropout par statut matrimonial\n",
    "marital_stats = (\n",
    "    df.groupby(\"Marital status\")\n",
    "    .agg({\"Dropout_Binary\": lambda x: (x == \"Dropout\").sum(), \"Target\": \"count\"})\n",
    "    .rename(columns={\"Dropout_Binary\": \"Dropouts\", \"Target\": \"Total\"})\n",
    ")\n",
    "\n",
    "marital_stats[\"Taux Dropout (%)\"] = (\n",
    "    marital_stats[\"Dropouts\"] / marital_stats[\"Total\"] * 100\n",
    ").round(1)\n",
    "marital_stats[\"Statut\"] = marital_stats.index.map(marital_mapping)\n",
    "marital_stats = marital_stats.sort_values(\"Taux Dropout (%)\", ascending=False)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique 1: Taux de dropout\n",
    "ax1 = axes[0]\n",
    "x_positions = range(len(marital_stats))\n",
    "colors = [\n",
    "    \"#e74c3c\" if x > global_dropout_rate else \"#2ecc71\" for x in marital_stats[\"Taux Dropout (%)\"]\n",
    "]\n",
    "bars1 = ax1.bar(x_positions, marital_stats[\"Taux Dropout (%)\"], color=colors, edgecolor=\"black\")\n",
    "ax1.axhline(y=global_dropout_rate, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "ax1.set_ylabel(\"Taux de Dropout (%)\")\n",
    "ax1.set_title(\"Taux de Dropout par Statut Matrimonial\", fontsize=12, weight=\"bold\")\n",
    "ax1.set_xticks(x_positions)\n",
    "ax1.set_xticklabels(marital_stats[\"Statut\"], rotation=45, ha=\"right\")\n",
    "\n",
    "# Graphique 2: Effectifs\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(x_positions, marital_stats[\"Total\"], color=\"steelblue\", edgecolor=\"black\")\n",
    "ax2.set_ylabel(\"Nombre d'√©tudiants\")\n",
    "ax2.set_title(\"Effectifs par Statut Matrimonial\", fontsize=12, weight=\"bold\")\n",
    "ax2.set_xticks(x_positions)\n",
    "ax2.set_xticklabels(marital_stats[\"Statut\"], rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau r√©capitulatif\n",
    "print(\"\\nüìã Tableau r√©capitulatif:\")\n",
    "display(marital_stats[[\"Statut\", \"Total\", \"Dropouts\", \"Taux Dropout (%)\"]].reset_index(drop=True))\n",
    "\n",
    "print(\n",
    "    f\"\\nüí° Note: La majorit√© des √©tudiants sont c√©libataires ({marital_stats[marital_stats['Statut'] == 'C√©libataire']['Total'].values[0]} sur {len(df)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Phase 4 : Analyse de Corr√©lation\n",
    "\n",
    "**Objectif** : Identifier les variables les plus corr√©l√©es au Dropout et d√©tecter les multicolin√©arit√©s.\n",
    "\n",
    "**Questions cl√©s** :\n",
    "- Quelles variables num√©riques sont les plus li√©es au Dropout ?\n",
    "- Y a-t-il des variables redondantes (fortement corr√©l√©es entre elles) ?\n",
    "- Quelles variables prioriser pour le machine learning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.1 Matrice de Corr√©lation Compl√®te\n",
    "# =============================================================================\n",
    "# Cr√©ation d'une variable num√©rique pour Dropout (1 = Dropout, 0 = Non-Dropout)\n",
    "df[\"Dropout_Numeric\"] = (df[\"Dropout_Binary\"] == \"Dropout\").astype(int)\n",
    "\n",
    "# S√©lection des variables num√©riques pour la corr√©lation\n",
    "# On exclut les variables cat√©gorielles encod√©es en num√©rique\n",
    "numeric_for_corr = [\n",
    "    \"Dropout_Numeric\",  # Notre variable cible\n",
    "    \"Age at enrollment\",\n",
    "    \"Admission grade\",\n",
    "    \"Previous qualification (grade)\",\n",
    "    # Performance 1er semestre\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\n",
    "    # Performance 2√®me semestre\n",
    "    \"Curricular units 2nd sem (credited)\",\n",
    "    \"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\n",
    "    # Indicateurs √©conomiques\n",
    "    \"Unemployment rate\",\n",
    "    \"Inflation rate\",\n",
    "    \"GDP\",\n",
    "]\n",
    "\n",
    "# Calcul de la matrice de corr√©lation\n",
    "corr_matrix = df[numeric_for_corr].corr()\n",
    "\n",
    "# Visualisation avec heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "# Masque pour la partie triangulaire sup√©rieure (√©vite les doublons)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Heatmap avec annotations\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",  # Rouge = corr√©lation n√©gative, Bleu = positive\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Coefficient de corr√©lation\"},\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Matrice de Corr√©lation - Variables Num√©riques vs Dropout\\n(Triangle inf√©rieur)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpr√©tation de la heatmap :\")\n",
    "print(\"‚Ä¢ üî¥ Rouge fonc√© : Corr√©lation n√©gative forte (quand l'une augmente, l'autre diminue)\")\n",
    "print(\"‚Ä¢ üîµ Bleu fonc√© : Corr√©lation positive forte (les deux augmentent ensemble)\")\n",
    "print(\"‚Ä¢ ‚ö™ Blanc/Clair : Pas de corr√©lation significative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.2 Top 10 Corr√©lations avec Dropout\n",
    "# =============================================================================\n",
    "# Extraction des corr√©lations avec notre variable cible\n",
    "dropout_corr = corr_matrix[\"Dropout_Numeric\"].drop(\"Dropout_Numeric\")\n",
    "\n",
    "# Tri par valeur absolue (pour voir les plus fortes, positives ou n√©gatives)\n",
    "dropout_corr_sorted = dropout_corr.reindex(dropout_corr.abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Affichage du Top 10\n",
    "print(\"üéØ Top 10 Variables les plus corr√©l√©es au Dropout\\n\")\n",
    "print(\"=\" * 60)\n",
    "for i, (var, corr) in enumerate(dropout_corr_sorted.head(10).items(), 1):\n",
    "    direction = \"‚¨ÜÔ∏è positive\" if corr > 0 else \"‚¨áÔ∏è n√©gative\"\n",
    "    print(f\"{i:2}. {var:<45} {corr:+.3f} ({direction})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualisation avec barplot horizontal\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = [\"#e74c3c\" if x > 0 else \"#2ecc71\" for x in dropout_corr_sorted.head(10)]\n",
    "bars = ax.barh(range(10), dropout_corr_sorted.head(10).values, color=colors)\n",
    "\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels(dropout_corr_sorted.head(10).index)\n",
    "ax.invert_yaxis()  # Top variables en haut\n",
    "ax.axvline(x=0, color=\"black\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Coefficient de Corr√©lation avec Dropout\")\n",
    "ax.set_title(\n",
    "    \"Top 10 Variables les Plus Corr√©l√©es au Dropout\\n(Rouge = ‚Üë Dropout, Vert = ‚Üì Dropout)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Annotations sur les barres\n",
    "for bar, val in zip(bars, dropout_corr_sorted.head(10).values):\n",
    "    ax.text(\n",
    "        val + 0.02 if val > 0 else val - 0.02,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{val:+.3f}\",\n",
    "        va=\"center\",\n",
    "        ha=\"left\" if val > 0 else \"right\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpr√©tation\n",
    "print(\"\\nüìä Interpr√©tation :\")\n",
    "print(\"\\nüî¥ Corr√©lations POSITIVES (facteurs de risque - augmentent le dropout) :\")\n",
    "for var, corr in dropout_corr_sorted.head(10).items():\n",
    "    if corr > 0.1:\n",
    "        print(f\"- {var}: {corr:+.3f}\")\n",
    "\n",
    "print(\"\\nüü¢ Corr√©lations N√âGATIVES (facteurs protecteurs - diminuent le dropout) :\")\n",
    "for var, corr in dropout_corr_sorted.head(10).items():\n",
    "    if corr < -0.1:\n",
    "        print(f\"- {var}: {corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.3 Analyse de Multicolin√©arit√©\n",
    "# =============================================================================\n",
    "# Recherche des paires de variables fortement corr√©l√©es entre elles\n",
    "# (potentiellement redondantes pour un mod√®le ML)\n",
    "\n",
    "# Cr√©ation d'un masque pour la partie triangulaire sup√©rieure (sans la diagonale)\n",
    "mask_upper = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Extraction des corr√©lations\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix)):\n",
    "    for j in range(i + 1, len(corr_matrix)):\n",
    "        var1 = corr_matrix.index[i]\n",
    "        var2 = corr_matrix.columns[j]\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if var1 != \"Dropout_Numeric\" and var2 != \"Dropout_Numeric\":\n",
    "            corr_pairs.append((var1, var2, corr_val))\n",
    "\n",
    "# Tri par corr√©lation absolue\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Affichage des paires les plus corr√©l√©es\n",
    "print(\"üîç Paires de Variables avec Forte Multicolin√©arit√© (|r| > 0.7)\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Variable 1':<40} {'Variable 2':<25} {'Corr√©lation':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "high_corr_count = 0\n",
    "for var1, var2, corr in corr_pairs:\n",
    "    if abs(corr) > 0.7:\n",
    "        print(f\"{var1:<40} {var2:<25} {corr:+.3f}\")\n",
    "        high_corr_count += 1\n",
    "\n",
    "if high_corr_count == 0:\n",
    "    print(\"Aucune paire avec |r| > 0.7\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Nombre de paires avec multicolin√©arit√© forte : {high_corr_count}\")\n",
    "\n",
    "# Recommandations\n",
    "print(\"\\nüí° Recommandations pour le Machine Learning :\")\n",
    "print(\"\\n1. Variables CL√âS (plus corr√©l√©es au Dropout) :\")\n",
    "for var, corr in dropout_corr_sorted.head(5).items():\n",
    "    print(f\"- {var}: {corr:+.3f}\")\n",
    "\n",
    "print(\"\\n2. Variables REDONDANTES (multicolin√©arit√©) :\")\n",
    "seen = set()\n",
    "for var1, var2, corr in corr_pairs[:5]:\n",
    "    if abs(corr) > 0.5:\n",
    "        if var1 not in seen and var2 not in seen:\n",
    "            print(f\"- {var1} ‚Üî {var2} (r={corr:+.3f})\")\n",
    "            print(\"     ‚Üí Consid√©rer de ne garder qu'une des deux\")\n",
    "            seen.add(var2)\n",
    "\n",
    "print(\"\\n3. Strat√©gie recommand√©e :\")\n",
    "print(\"- Prioriser les variables de performance (notes, unit√©s valid√©es)\")\n",
    "print(\"- Attention : les variables du 1er et 2√®me semestre sont tr√®s corr√©l√©es\")\n",
    "print(\"- L'√¢ge est le seul facteur de risque positif majeur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Synth√®se Phase 4 : Matrice de Corr√©lation\n",
    "\n",
    "**D√©couvertes cl√©s :**\n",
    "\n",
    "| Aspect | R√©sultat |\n",
    "|--------|----------|\n",
    "| **Pr√©dicteurs les plus forts** | Notes et unit√©s valid√©es du 2√®me semestre (r ‚âà -0.57) |\n",
    "| **Facteur de risque positif** | √Çge √† l'inscription (+0.254) - √©tudiants plus √¢g√©s |\n",
    "| **Indicateurs √©conomiques** | Faible corr√©lation avec le dropout (< 0.05) |\n",
    "| **Multicolin√©arit√© d√©tect√©e** | 11 paires avec \\|r\\| > 0.7 |\n",
    "\n",
    "**Implications pour le Machine Learning :**\n",
    "1. Les variables sem1 et sem2 sont fortement corr√©l√©es ‚Üí risque de redondance\n",
    "2. Prioriser les **grades** (notes) sur les autres m√©triques acad√©miques\n",
    "3. Le 2√®me semestre est plus pr√©dictif que le 1er\n",
    "4. L'√¢ge est un facteur de risque √† ne pas n√©gliger\n",
    "\n",
    "**Prochaine √©tape sugg√©r√©e :** Feature Engineering (cr√©er des ratios, agr√©gations) pour r√©duire la dimensionnalit√© tout en conservant l'information pr√©dictive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Phase 5 : Feature Engineering\n",
    "\n",
    "**Objectif** : Cr√©er de nouvelles variables pour am√©liorer la pr√©diction et r√©duire la multicolin√©arit√© d√©tect√©e en Phase 4.\n",
    "\n",
    "**Transformations pr√©vues :**\n",
    "| Type | Description |\n",
    "|------|-------------|\n",
    "| **Discr√©tisation** | √Çge ‚Üí tranches (17-20, 21-25, 26-35, 36+) |\n",
    "| **Regroupement** | Statut marital ‚Üí Solo/Couple |\n",
    "| **Regroupement** | Qualifications ‚Üí Secondaire/Sup√©rieur |\n",
    "| **Regroupement** | Programmes ‚Üí Domaines (Sant√©, Tech, Business...) |\n",
    "| **Ratios** | Taux de r√©ussite par semestre |\n",
    "| **Agr√©gations** | Notes moyennes, tendance de performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.1 Discr√©tisation de l'√¢ge\n",
    "# =============================================================================\n",
    "# Objectif : Transformer l'√¢ge continu en cat√©gories pour faciliter l'analyse\n",
    "# Justification : L'√¢ge a une corr√©lation positive avec le dropout (+0.254)\n",
    "\n",
    "# D√©finition des tranches d'√¢ge (labels purement descriptifs)\n",
    "bins = [0, 20, 25, 35, 100]\n",
    "labels = [\"17-20\", \"21-25\", \"26-35\", \"36+\"]\n",
    "\n",
    "# Cr√©ation de la nouvelle variable\n",
    "df[\"Age_Group\"] = pd.cut(df[\"Age at enrollment\"], bins=bins, labels=labels)\n",
    "\n",
    "# V√©rification de la distribution\n",
    "print(\"üìä Distribution des tranches d'√¢ge:\")\n",
    "print(\"=\" * 50)\n",
    "age_dist = df[\"Age_Group\"].value_counts().sort_index()\n",
    "for group, count in age_dist.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {group}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Calcul du taux de dropout par tranche d'√¢ge (avec observed=True pour √©viter le FutureWarning)\n",
    "print(\"\\nüéØ Taux de Dropout par tranche d'√¢ge:\")\n",
    "print(\"=\" * 50)\n",
    "dropout_by_age = (\n",
    "    df.groupby(\"Age_Group\", observed=True)[\"Dropout_Binary\"]\n",
    "    .apply(lambda x: (x == \"Dropout\").mean() * 100)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "for group, rate in dropout_by_age.items():\n",
    "    indicator = \"üî¥\" if rate > 35 else \"üü°\" if rate > 30 else \"üü¢\"\n",
    "    print(f\"  {indicator} {group}: {rate:.1f}%\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution des tranches d'√¢ge\n",
    "colors = [\"#3498db\", \"#9b59b6\", \"#e67e22\", \"#e74c3c\"]\n",
    "ax1 = axes[0]\n",
    "age_dist.plot(kind=\"bar\", ax=ax1, color=colors, edgecolor=\"black\", alpha=0.8)\n",
    "ax1.set_title(\"Distribution des Tranches d'√Çge\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Tranche d'√¢ge\")\n",
    "ax1.set_ylabel(\"Nombre d'√©tudiants\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Ajouter les pourcentages sur les barres\n",
    "for i, (idx, val) in enumerate(age_dist.items()):\n",
    "    ax1.text(i, val + 50, f\"{val / len(df) * 100:.1f}%\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Taux de dropout par tranche\n",
    "ax2 = axes[1]\n",
    "colors_dropout = [\n",
    "    \"#2ecc71\" if r < 30 else \"#f39c12\" if r < 35 else \"#e74c3c\" for r in dropout_by_age\n",
    "]\n",
    "dropout_by_age.plot(kind=\"bar\", ax=ax2, color=colors_dropout, edgecolor=\"black\", alpha=0.8)\n",
    "ax2.axhline(\n",
    "    y=df[\"Dropout_Binary\"].apply(lambda x: x == \"Dropout\").mean() * 100,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Moyenne globale\",\n",
    ")\n",
    "ax2.set_title(\"Taux de Dropout par Tranche d'√Çge\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Tranche d'√¢ge\")\n",
    "ax2.set_ylabel(\"Taux de Dropout (%)\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, (idx, val) in enumerate(dropout_by_age.items()):\n",
    "    ax2.text(i, val + 1, f\"{val:.1f}%\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight : Les √©tudiants plus √¢g√©s (26+) ont un taux de dropout plus √©lev√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.7 Tableau r√©capitulatif des nouvelles features\n",
    "# =============================================================================\n",
    "\n",
    "# Recr√©er les features manquantes si n√©cessaire (en cas de red√©marrage kernel)\n",
    "if \"Marital_Binary\" not in df.columns:\n",
    "    solo = [1, 3, 4, 6]  # C√©libataire, Veuf, Divorc√©, S√©par√©\n",
    "    df[\"Marital_Binary\"] = df[\"Marital status\"].apply(lambda x: \"Solo\" if x in solo else \"Couple\")\n",
    "\n",
    "if \"Education_Level\" not in df.columns:\n",
    "    secondaire = [1, 9, 10, 12, 14, 15, 19, 38]\n",
    "    superieur = [2, 3, 4, 5, 6, 39, 40, 42, 43]\n",
    "    df[\"Education_Level\"] = df[\"Previous qualification\"].apply(\n",
    "        lambda x: \"Secondaire\" if x in secondaire else \"Sup√©rieur\" if x in superieur else \"Autre\"\n",
    "    )\n",
    "\n",
    "if \"Course_Domain\" not in df.columns:\n",
    "    course_domains = {\n",
    "        33: \"Tech\",\n",
    "        171: \"Arts\",\n",
    "        8014: \"Social\",\n",
    "        9003: \"Sciences\",\n",
    "        9070: \"Arts\",\n",
    "        9085: \"Sant√©\",\n",
    "        9119: \"Tech\",\n",
    "        9130: \"Sciences\",\n",
    "        9147: \"Business\",\n",
    "        9238: \"Social\",\n",
    "        9254: \"Business\",\n",
    "        9500: \"Sant√©\",\n",
    "        9556: \"Sant√©\",\n",
    "        9670: \"Business\",\n",
    "        9773: \"Arts\",\n",
    "        9853: \"Education\",\n",
    "        9991: \"Business\",\n",
    "    }\n",
    "    df[\"Course_Domain\"] = df[\"Course\"].map(course_domains)\n",
    "\n",
    "if \"Success_Rate_Sem1\" not in df.columns:\n",
    "    df[\"Success_Rate_Sem1\"] = df[\"Curricular units 1st sem (approved)\"] / df[\n",
    "        \"Curricular units 1st sem (enrolled)\"\n",
    "    ].replace(0, np.nan)\n",
    "    df[\"Success_Rate_Sem2\"] = df[\"Curricular units 2nd sem (approved)\"] / df[\n",
    "        \"Curricular units 2nd sem (enrolled)\"\n",
    "    ].replace(0, np.nan)\n",
    "\n",
    "if \"Avg_Grade\" not in df.columns:\n",
    "    df[\"Avg_Grade\"] = (\n",
    "        df[\"Curricular units 1st sem (grade)\"] + df[\"Curricular units 2nd sem (grade)\"]\n",
    "    ) / 2\n",
    "    df[\"Total_Approved\"] = (\n",
    "        df[\"Curricular units 1st sem (approved)\"] + df[\"Curricular units 2nd sem (approved)\"]\n",
    "    )\n",
    "    df[\"Performance_Trend\"] = (\n",
    "        df[\"Curricular units 2nd sem (grade)\"] - df[\"Curricular units 1st sem (grade)\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## R√©capitulatif des nouvelles features cr√©√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des nouvelles features avec leurs statistiques\n",
    "new_features = [\n",
    "    (\"Age_Group\", \"Cat√©gorielle\", \"4 cat√©gories\"),\n",
    "    (\"Marital_Binary\", \"Binaire\", \"Solo / Couple\"),\n",
    "    (\"Education_Level\", \"Cat√©gorielle\", \"3 niveaux\"),\n",
    "    (\"Course_Domain\", \"Cat√©gorielle\", \"6 domaines\"),\n",
    "    (\"Success_Rate_Sem1\", \"Num√©rique\", \"Ratio [0-1]\"),\n",
    "    (\"Success_Rate_Sem2\", \"Num√©rique\", \"Ratio [0-1]\"),\n",
    "    (\"Avg_Grade\", \"Num√©rique\", \"Moyenne notes\"),\n",
    "    (\"Total_Approved\", \"Num√©rique\", \"Somme unit√©s\"),\n",
    "    (\"Performance_Trend\", \"Num√©rique\", \"Diff√©rence grades\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Feature':<25} {'Type':<15} {'Description':<20}\")\n",
    "print(\"-\" * 60)\n",
    "for name, ftype, desc in new_features:\n",
    "    print(f\"{name:<25} {ftype:<15} {desc:<20}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Impact sur le dropout pour chaque nouvelle feature\n",
    "print(\"üéØ IMPACT DES NOUVELLES FEATURES SUR LE DROPOUT\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Features cat√©gorielles\n",
    "print(\"VARIABLES CAT√âGORIELLES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for feat in [\"Age_Group\", \"Marital_Binary\", \"Education_Level\", \"Course_Domain\"]:\n",
    "    dropout_rate = df.groupby(feat, observed=True)[\"Dropout_Binary\"].apply(\n",
    "        lambda x: (x == \"Dropout\").mean() * 100\n",
    "    )\n",
    "    max_rate = dropout_rate.max()\n",
    "    min_rate = dropout_rate.min()\n",
    "    spread = max_rate - min_rate\n",
    "    print(f\"  {feat}: √©cart {spread:.1f}% (min: {min_rate:.1f}%, max: {max_rate:.1f}%)\")\n",
    "\n",
    "print()\n",
    "print(\"VARIABLES NUM√âRIQUES (corr√©lation avec Dropout):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for feat in [\n",
    "    \"Success_Rate_Sem1\",\n",
    "    \"Success_Rate_Sem2\",\n",
    "    \"Avg_Grade\",\n",
    "    \"Total_Approved\",\n",
    "    \"Performance_Trend\",\n",
    "]:\n",
    "    corr = df[feat].corr(df[\"Dropout_Numeric\"])\n",
    "    direction = \"üî¥ Risque\" if corr > 0 else \"üü¢ Protecteur\"\n",
    "    print(f\"  {feat}: r = {corr:+.3f} ({direction})\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"üí° Les nouvelles features am√©liorent la lisibilit√© et r√©duisent la multicolin√©arit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Synth√®se Phase 5 : Feature Engineering\n",
    "\n",
    "**9 nouvelles variables cr√©√©es** pour am√©liorer l'analyse et pr√©parer le Machine Learning.\n",
    "\n",
    "#### R√©sultats cl√©s :\n",
    "\n",
    "| Type de transformation | Meilleure feature | Impact sur Dropout |\n",
    "|------------------------|-------------------|-------------------|\n",
    "| **Discr√©tisation** | Age_Group | √âcart de 36.3% entre tranches |\n",
    "| **Regroupement cat√©goriel** | Course_Domain | √âcart de 34.7% entre domaines |\n",
    "| **Ratio de performance** | Success_Rate_Sem2 | r = -0.705 (le plus fort !) |\n",
    "| **Agr√©gation** | Avg_Grade | r = -0.551 |\n",
    "\n",
    "#### D√©couvertes importantes :\n",
    "\n",
    "1. **Le taux de r√©ussite est plus pr√©dictif que les notes brutes**\n",
    "   - `Success_Rate_Sem2` (r = -0.705) surpasse m√™me les notes individuelles\n",
    "   - Cela confirme l'importance du ratio approved/enrolled\n",
    "\n",
    "2. **Les domaines d'√©tudes r√©v√®lent des patterns clairs**\n",
    "   - Tech : 54.9% de dropout (risque tr√®s √©lev√©)\n",
    "   - Sant√© : 20.3% de dropout (protection forte)\n",
    "\n",
    "3. **Les regroupements simplifient sans perdre d'information**\n",
    "   - Solo vs Couple capture l'essentiel du statut marital\n",
    "   - Secondaire/Sup√©rieur est suffisant pour la qualification\n",
    "\n",
    "#### Recommandations pour le Modeling :\n",
    "\n",
    "‚úÖ **Variables √† privil√©gier** :\n",
    "- `Success_Rate_Sem2` (r = -0.705)\n",
    "- `Age_Group` (√©cart 36.3%)\n",
    "- `Course_Domain` (√©cart 34.7%)\n",
    "\n",
    "‚ùå **Variables redondantes √† √©viter** :\n",
    "- Ne pas utiliser √† la fois les notes ET les ratios\n",
    "- Choisir entre `Total_Approved` et `Success_Rate`\n",
    "\n",
    "---\n",
    "\n",
    "**L'EDA est maintenant complet !** Les donn√©es sont pr√™tes pour la phase de mod√©lisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Phase 6 : Pr√©paration ML et Mod√©lisation\n",
    "\n",
    "**Objectif** : Pr√©parer les donn√©es et entra√Æner des mod√®les de classification pour pr√©dire le dropout.\n",
    "\n",
    "### Strat√©gie bas√©e sur l'EDA :\n",
    "| Aspect | D√©cision |\n",
    "|--------|----------|\n",
    "| **Features cl√©s** | Ratios de performance (`Success_Rate_Sem2` r=-0.705), agr√©gations |\n",
    "| **Exclusions** | Variables avec multicolin√©arit√© (grades individuels), indicateurs √©conomiques |\n",
    "| **D√©s√©quilibre** | ~32% Dropout ‚Üí utiliser `class_weight='balanced'` |\n",
    "| **M√©trique prioritaire** | **Recall** (ne pas manquer les √©tudiants √† risque) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.1 Pr√©paration des Features et Target\n",
    "# =============================================================================\n",
    "# S√©lection des features bas√©e sur l'EDA (corr√©lations, multicolin√©arit√©)\n",
    "\n",
    "# Recr√©er les features engineered si n√©cessaire\n",
    "if \"Success_Rate_Sem1\" not in df.columns:\n",
    "    df[\"Success_Rate_Sem1\"] = df[\"Curricular units 1st sem (approved)\"] / df[\n",
    "        \"Curricular units 1st sem (enrolled)\"\n",
    "    ].replace(0, np.nan)\n",
    "    df[\"Success_Rate_Sem2\"] = df[\"Curricular units 2nd sem (approved)\"] / df[\n",
    "        \"Curricular units 2nd sem (enrolled)\"\n",
    "    ].replace(0, np.nan)\n",
    "    df[\"Avg_Grade\"] = (\n",
    "        df[\"Curricular units 1st sem (grade)\"] + df[\"Curricular units 2nd sem (grade)\"]\n",
    "    ) / 2\n",
    "    df[\"Total_Approved\"] = (\n",
    "        df[\"Curricular units 1st sem (approved)\"] + df[\"Curricular units 2nd sem (approved)\"]\n",
    "    )\n",
    "    df[\"Performance_Trend\"] = (\n",
    "        df[\"Curricular units 2nd sem (grade)\"] - df[\"Curricular units 1st sem (grade)\"]\n",
    "    )\n",
    "\n",
    "# D√©finir les features selon le plan\n",
    "numeric_features = [\n",
    "    \"Success_Rate_Sem1\",\n",
    "    \"Success_Rate_Sem2\",\n",
    "    \"Avg_Grade\",\n",
    "    \"Total_Approved\",\n",
    "    \"Age at enrollment\",\n",
    "    \"Admission grade\",\n",
    "    \"Performance_Trend\",\n",
    "]\n",
    "\n",
    "# Features cat√©gorielles cr√©√©es pendant le Feature Engineering\n",
    "categorical_features = [\n",
    "    \"Age_Group\",\n",
    "    \"Course_Domain\",\n",
    "    \"Marital_Binary\",\n",
    "    \"Education_Level\",\n",
    "]\n",
    "\n",
    "# Features binaires (d√©j√† encod√©es 0/1)\n",
    "binary_features = [\n",
    "    \"Tuition fees up to date\",\n",
    "    \"Scholarship holder\",\n",
    "    \"Debtor\",\n",
    "    \"Gender\",\n",
    "    \"Displaced\",\n",
    "]\n",
    "\n",
    "# V√©rifier que toutes les features existent\n",
    "all_features = numeric_features + categorical_features + binary_features\n",
    "missing_cols = [col for col in all_features if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è Colonnes manquantes : {missing_cols}\")\n",
    "else:\n",
    "    print(\"‚úÖ Toutes les features sont disponibles\")\n",
    "\n",
    "# Cr√©er X (features) et y (target binaire)\n",
    "X = df[all_features].copy()\n",
    "y = (df[\"Dropout_Binary\"] == \"Dropout\").astype(int)  # 1 = Dropout, 0 = Non-Dropout\n",
    "\n",
    "print(\"\\nDimensions:\")\n",
    "print(f\"  X : {X.shape}\")\n",
    "print(f\"  y : {y.shape}\")\n",
    "print(\"\\nDistribution du target:\")\n",
    "print(f\"  Dropout (1)     : {y.sum()} ({y.mean() * 100:.1f}%)\")\n",
    "print(f\"  Non-Dropout (0) : {len(y) - y.sum()} ({(1 - y.mean()) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2 Gestion des Valeurs Manquantes\n",
    "# =============================================================================\n",
    "# Les ratios Success_Rate peuvent avoir des NaN (division par 0 si enrolled=0)\n",
    "\n",
    "print(\"üìä Valeurs manquantes dans X:\")\n",
    "print(\"=\" * 50)\n",
    "missing = X.isnull().sum()\n",
    "missing_pct = (X.isnull().sum() / len(X)) * 100\n",
    "missing_df = pd.DataFrame({\"Manquantes\": missing, \"Pourcentage\": missing_pct})\n",
    "missing_df = missing_df[missing_df[\"Manquantes\"] > 0].sort_values(\"Manquantes\", ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "\n",
    "    # Strat√©gie : Imputer avec la m√©diane pour les variables num√©riques\n",
    "    # (les NaN signifient 0 unit√©s inscrites = √©tudiant probablement en difficult√©)\n",
    "    print(\"\\nüîß Strat√©gie d'imputation : M√©diane pour les variables num√©riques\")\n",
    "\n",
    "    for col in missing_df.index:\n",
    "        if col in numeric_features:\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "            print(f\"  {col}: NaN ‚Üí {median_val:.3f} (m√©diane)\")\n",
    "\n",
    "    print(\"\\n‚úÖ Apr√®s imputation:\")\n",
    "    print(f\"  Valeurs manquantes restantes: {X.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"‚úÖ Aucune valeur manquante!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.3 Pipeline de Pr√©traitement et Split Train/Test\n",
    "# =============================================================================\n",
    "\n",
    "# Cr√©er le preprocessor avec ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"),\n",
    "            categorical_features,\n",
    "        ),\n",
    "        (\"bin\", \"passthrough\", binary_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# Split stratifi√© (pr√©serve les proportions de classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìä Split Train/Test (stratifi√©):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Train: {len(X_train)} samples ({len(X_train) / len(X) * 100:.0f}%)\")\n",
    "print(f\"    - Dropout: {y_train.sum()} ({y_train.mean() * 100:.1f}%)\")\n",
    "print(f\"    - Non-Dropout: {len(y_train) - y_train.sum()} ({(1 - y_train.mean()) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Test: {len(X_test)} samples ({len(X_test) / len(X) * 100:.0f}%)\")\n",
    "print(f\"    - Dropout: {y_test.sum()} ({y_test.mean() * 100:.1f}%)\")\n",
    "print(f\"    - Non-Dropout: {len(y_test) - y_test.sum()} ({(1 - y_test.mean()) * 100:.1f}%)\")\n",
    "\n",
    "# V√©rifier que les proportions sont pr√©serv√©es\n",
    "print(\n",
    "    f\"\\n‚úÖ Proportions pr√©serv√©es: Train {y_train.mean() * 100:.1f}% vs Test {y_test.mean() * 100:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.4 Mod√®le Baseline : Logistic Regression\n",
    "# =============================================================================\n",
    "# Pourquoi Logistic Regression comme baseline ?\n",
    "# - Interpr√©table (coefficients = importance des features)\n",
    "# - Rapide √† entra√Æner\n",
    "# - R√©f√©rence pour comparer les mod√®les plus complexes\n",
    "\n",
    "# Cr√©er le pipeline complet (pr√©traitement + mod√®le)\n",
    "baseline_model = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(\n",
    "                class_weight=\"balanced\",  # G√®re le d√©s√©quilibre des classes\n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "print(\"üîÑ Entra√Ænement du mod√®le baseline (Logistic Regression)...\")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Mod√®le entra√Æn√©!\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©triques\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä √âVALUATION DU MOD√àLE BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã Rapport de Classification:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-Dropout\", \"Dropout\"]))\n",
    "\n",
    "# M√©triques cl√©s\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\nüéØ M√©triques Prioritaires (classe Dropout):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Recall (Dropout)  : {recall:.3f} ‚≠ê (priorit√© : ne pas manquer les √† risque)\")\n",
    "print(f\"  F1-Score (Dropout): {f1:.3f}\")\n",
    "print(f\"  AUC-ROC           : {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.5 Visualisation des R√©sultats\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Matrice de confusion\n",
    "ax1 = axes[0]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-Dropout\", \"Dropout\"])\n",
    "disp.plot(ax=ax1, cmap=\"Blues\", values_format=\"d\")\n",
    "ax1.set_title(\n",
    "    \"Matrice de Confusion\\n(Logistic Regression Baseline)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Courbe ROC\n",
    "ax2 = axes[1]\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred_proba, ax=ax2, color=\"#3498db\", lw=2)\n",
    "ax2.plot([0, 1], [0, 1], \"k--\", lw=1, label=\"Random (AUC=0.5)\")\n",
    "ax2.set_title(f\"Courbe ROC (AUC = {auc:.3f})\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpr√©tation de la matrice de confusion\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nüìä Interpr√©tation de la Matrice de Confusion:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  ‚úÖ Vrais N√©gatifs (TN)  : {tn} Non-Dropouts correctement identifi√©s\")\n",
    "print(f\"  ‚úÖ Vrais Positifs (TP)  : {tp} Dropouts correctement identifi√©s\")\n",
    "print(f\"  ‚ö†Ô∏è Faux Positifs (FP)   : {fp} Non-Dropouts class√©s Dropout (fausse alerte)\")\n",
    "print(f\"  ‚ùå Faux N√©gatifs (FN)   : {fn} Dropouts manqu√©s (risque principal!)\")\n",
    "print(\n",
    "    f\"\\nüí° Sur {y_test.sum()} √©tudiants √† risque, le mod√®le en identifie {tp} ({tp / y_test.sum() * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Synth√®se Phase 6 : Mod√®le Baseline\n",
    "\n",
    "**R√©sultats du mod√®le Logistic Regression (Baseline)** :\n",
    "\n",
    "| M√©trique | Valeur | Interpr√©tation |\n",
    "|----------|--------|----------------|\n",
    "| **Recall (Dropout)** | 83.1% | Sur 284 √©tudiants √† risque, 236 sont identifi√©s |\n",
    "| **F1-Score (Dropout)** | 81.4% | Bon √©quilibre pr√©cision/rappel |\n",
    "| **AUC-ROC** | 93.1% | Excellente capacit√© de discrimination |\n",
    "| **Accuracy** | 87.8% | Performance globale |\n",
    "\n",
    "**Analyse des erreurs** :\n",
    "- **48 Faux N√©gatifs** : √âtudiants √† risque non d√©tect√©s (erreur critique ‚ùå)\n",
    "- **60 Faux Positifs** : Fausses alertes (co√ªt acceptable)\n",
    "\n",
    "**Conclusion** :\n",
    "Le mod√®le baseline Logistic Regression atteint d√©j√† des performances tr√®s satisfaisantes gr√¢ce :\n",
    "1. Aux features engineered (`Success_Rate_Sem2` avec r = -0.705)\n",
    "2. √Ä la gestion du d√©s√©quilibre via `class_weight='balanced'`\n",
    "3. √Ä un bon pr√©traitement (StandardScaler + OneHotEncoder)\n",
    "\n",
    "**Prochaines √©tapes sugg√©r√©es** :\n",
    "1. Tester d'autres mod√®les (Random Forest, XGBoost, etc.)\n",
    "2. Optimiser les hyperparam√®tres avec GridSearchCV\n",
    "3. Explorer SMOTE vs class_weight pour le d√©s√©quilibre\n",
    "4. Ajuster le seuil de d√©cision pour maximiser le Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Phase 6.2 : Comparaison de Mod√®les et Optimisation\n",
    "\n",
    "**Objectif** : Tester plusieurs algorithmes et optimiser le meilleur mod√®le.\n",
    "\n",
    "### Mod√®les √† comparer :\n",
    "| Niveau | Mod√®le | Caract√©ristiques |\n",
    "|--------|--------|------------------|\n",
    "| Baseline | Logistic Regression | D√©j√† fait (Recall=83.1%, AUC=93.1%) |\n",
    "| Interm√©diaire | Random Forest | Robuste, feature importance native |\n",
    "| Interm√©diaire | Gradient Boosting | Performant, g√®re bien le d√©s√©quilibre |\n",
    "| Interm√©diaire | SVM (RBF) | Bon pour donn√©es non-lin√©aires |\n",
    "| Avanc√© | XGBoost | √âtat de l'art, tr√®s performant |\n",
    "\n",
    "### Strat√©gie d'√©valuation :\n",
    "- **Cross-validation 5-fold stratifi√©e** pour comparer √©quitablement\n",
    "- **M√©trique principale** : F1-Score (√©quilibre pr√©cision/rappel)\n",
    "- **M√©trique secondaire** : Recall (ne pas manquer les dropouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Comparaison de Mo√®les avec Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer scale_pos_weight pour XGBoost (ratio classe majoritaire/minoritaire)\n",
    "scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "\n",
    "# D√©finir les mod√®les √† comparer\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        class_weight=\"balanced\", max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM (RBF)\": SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=True, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        verbosity=0,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"\\nMod√®les √† comparer : {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2.2 Cross-Validation 5-Fold Stratifi√©e\n",
    "# =============================================================================\n",
    "# Stratification : pr√©serve les proportions de classes dans chaque fold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Stocker les r√©sultats\n",
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"F1 Mean\": [],\n",
    "    \"F1 Std\": [],\n",
    "    \"Recall Mean\": [],\n",
    "    \"Recall Std\": [],\n",
    "    \"AUC Mean\": [],\n",
    "    \"AUC Std\": [],\n",
    "}\n",
    "\n",
    "print(\"üîÑ Cross-validation en cours...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Cr√©er le pipeline pour chaque mod√®le\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "    # Calculer les scores\n",
    "    f1_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=\"f1\", n_jobs=-1)\n",
    "    recall_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=\"recall\", n_jobs=-1)\n",
    "    auc_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "\n",
    "    # Stocker les r√©sultats\n",
    "    results[\"Model\"].append(name)\n",
    "    results[\"F1 Mean\"].append(f1_scores.mean())\n",
    "    results[\"F1 Std\"].append(f1_scores.std())\n",
    "    results[\"Recall Mean\"].append(recall_scores.mean())\n",
    "    results[\"Recall Std\"].append(recall_scores.std())\n",
    "    results[\"AUC Mean\"].append(auc_scores.mean())\n",
    "    results[\"AUC Std\"].append(auc_scores.std())\n",
    "\n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    print(f\"   F1-Score : {f1_scores.mean():.3f} (¬±{f1_scores.std():.3f})\")\n",
    "    print(f\"   Recall   : {recall_scores.mean():.3f} (¬±{recall_scores.std():.3f})\")\n",
    "    print(f\"   AUC-ROC  : {auc_scores.mean():.3f} (¬±{auc_scores.std():.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Cross-validation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2.3 Tableau Comparatif des Mod√®les\n",
    "# =============================================================================\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Trier par F1-Score\n",
    "results_df = results_df.sort_values(\"F1 Mean\", ascending=False)\n",
    "\n",
    "\n",
    "# Formater pour l'affichage\n",
    "def format_score(mean, std):\n",
    "    return f\"{mean:.3f} ¬±{std:.3f}\"\n",
    "\n",
    "\n",
    "display_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Mod√®le\": results_df[\"Model\"],\n",
    "        \"F1-Score\": [\n",
    "            format_score(m, s) for m, s in zip(results_df[\"F1 Mean\"], results_df[\"F1 Std\"])\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            format_score(m, s) for m, s in zip(results_df[\"Recall Mean\"], results_df[\"Recall Std\"])\n",
    "        ],\n",
    "        \"AUC-ROC\": [\n",
    "            format_score(m, s) for m, s in zip(results_df[\"AUC Mean\"], results_df[\"AUC Std\"])\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üìä Comparaison des Mod√®les (Cross-Validation 5-Fold)\")\n",
    "print(\"=\" * 70)\n",
    "display(display_df.reset_index(drop=True))\n",
    "\n",
    "# Identifier le meilleur mod√®le\n",
    "best_model_name = results_df.iloc[0][\"Model\"]\n",
    "best_f1 = results_df.iloc[0][\"F1 Mean\"]\n",
    "best_recall = results_df.iloc[0][\"Recall Mean\"]\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur mod√®le (F1-Score) : {best_model_name}\")\n",
    "print(f\"   F1 = {best_f1:.3f}, Recall = {best_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2.4 Visualisation Comparative\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Pr√©parer les donn√©es pour les graphiques\n",
    "models_names = results_df[\"Model\"].tolist()\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#e74c3c\", \"#9b59b6\"]\n",
    "\n",
    "# Graphique 1: F1-Score\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(\n",
    "    models_names,\n",
    "    results_df[\"F1 Mean\"],\n",
    "    xerr=results_df[\"F1 Std\"],\n",
    "    color=colors,\n",
    "    alpha=0.8,\n",
    "    capsize=5,\n",
    ")\n",
    "ax1.set_xlabel(\"F1-Score\", fontsize=12)\n",
    "ax1.set_title(\"F1-Score par Mod√®le\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_xlim(0.65, 0.85)\n",
    "for i, (v, s) in enumerate(zip(results_df[\"F1 Mean\"], results_df[\"F1 Std\"])):\n",
    "    ax1.text(v + s + 0.005, i, f\"{v:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Graphique 2: Recall\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.barh(\n",
    "    models_names,\n",
    "    results_df[\"Recall Mean\"],\n",
    "    xerr=results_df[\"Recall Std\"],\n",
    "    color=colors,\n",
    "    alpha=0.8,\n",
    "    capsize=5,\n",
    ")\n",
    "ax2.set_xlabel(\"Recall\", fontsize=12)\n",
    "ax2.set_title(\"Recall (Dropout) par Mod√®le\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_xlim(0.65, 0.85)\n",
    "for i, (v, s) in enumerate(zip(results_df[\"Recall Mean\"], results_df[\"Recall Std\"])):\n",
    "    ax2.text(v + s + 0.005, i, f\"{v:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Graphique 3: AUC-ROC\n",
    "ax3 = axes[2]\n",
    "bars3 = ax3.barh(\n",
    "    models_names,\n",
    "    results_df[\"AUC Mean\"],\n",
    "    xerr=results_df[\"AUC Std\"],\n",
    "    color=colors,\n",
    "    alpha=0.8,\n",
    "    capsize=5,\n",
    ")\n",
    "ax3.set_xlabel(\"AUC-ROC\", fontsize=12)\n",
    "ax3.set_title(\"AUC-ROC par Mod√®le\", fontsize=14, fontweight=\"bold\")\n",
    "ax3.set_xlim(0.85, 0.95)\n",
    "for i, (v, s) in enumerate(zip(results_df[\"AUC Mean\"], results_df[\"AUC Std\"])):\n",
    "    ax3.text(v + s + 0.002, i, f\"{v:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation : Les performances sont tr√®s proches entre les mod√®les.\")\n",
    "print(\n",
    "    \"   La Logistic Regression a le meilleur Recall (79.9%), importante pour d√©tecter les dropouts.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### 6.2.5 Optimisation des Hyperparam√®tres\n",
    "\n",
    "**R√©sultats de la comparaison** :\n",
    "- Les 4 mod√®les ont des performances tr√®s similaires (F1 entre 0.77 et 0.79)\n",
    "- **Logistic Regression** a le meilleur **Recall** (79.9%) - crucial pour notre objectif\n",
    "- **SVM (RBF)** a le meilleur **F1-Score** (79.0%)\n",
    "\n",
    "**Choix** : Nous allons optimiser **Logistic Regression** car :\n",
    "1. Meilleur Recall (priorit√© pour d√©tecter les dropouts)\n",
    "2. Plus interpr√©table (coefficients analysables)\n",
    "3. Plus rapide √† entra√Æner et d√©ployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2.6 GridSearchCV pour Logistic Regression\n",
    "# =============================================================================\n",
    "\n",
    "# Grille d'hyperparam√®tres pour Logistic Regression\n",
    "param_grid = {\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100],  # Force de r√©gularisation (inverse)\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],  # Type de r√©gularisation\n",
    "    \"classifier__solver\": [\"saga\"],  # Solver compatible avec l1 et l2\n",
    "    \"classifier__class_weight\": [\"balanced\"],  # Gestion du d√©s√©quilibre\n",
    "    \"classifier__max_iter\": [2000],  # Convergence\n",
    "}\n",
    "\n",
    "# Pipeline de base\n",
    "base_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# GridSearchCV avec F1-Score comme m√©trique\n",
    "print(\"üîÑ Optimisation des hyperparam√®tres en cours...\")\n",
    "print(\"   (GridSearchCV avec 5-fold cross-validation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    base_pipeline,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Optimisation termin√©e!\")\n",
    "print(\"\\nüèÜ Meilleurs hyperparam√®tres:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"   {param.replace('classifier__', '')}: {value}\")\n",
    "print(f\"\\nüìä Meilleur F1-Score (CV): {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2.7 √âvaluation du Mod√®le Optimis√© sur le Test Set\n",
    "# =============================================================================\n",
    "\n",
    "# Le meilleur mod√®le est d√©j√† r√©entra√Æn√© sur tout le train set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Pr√©dictions sur le test set\n",
    "y_pred_opt = best_model.predict(X_test)\n",
    "y_pred_proba_opt = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©triques\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä √âVALUATION DU MOD√àLE OPTIMIS√â (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã Rapport de Classification:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_pred_opt, target_names=[\"Non-Dropout\", \"Dropout\"]))\n",
    "\n",
    "# M√©triques cl√©s\n",
    "recall_opt = recall_score(y_test, y_pred_opt)\n",
    "f1_opt = f1_score(y_test, y_pred_opt)\n",
    "auc_opt = roc_auc_score(y_test, y_pred_proba_opt)\n",
    "\n",
    "print(\"\\nüéØ M√©triques Prioritaires (classe Dropout):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Recall (Dropout)  : {recall_opt:.3f}\")\n",
    "print(f\"  F1-Score (Dropout): {f1_opt:.3f}\")\n",
    "print(f\"  AUC-ROC           : {auc_opt:.3f}\")\n",
    "\n",
    "# Comparaison avec le baseline\n",
    "print(\"\\nüìà Comparaison avec le Baseline:\")\n",
    "print(\"-\" * 40)\n",
    "print(\n",
    "    f\"  Recall    : {recall:.3f} ‚Üí {recall_opt:.3f} ({'+' if recall_opt >= recall else ''}{(recall_opt - recall) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  F1-Score  : {f1:.3f} ‚Üí {f1_opt:.3f} ({'+' if f1_opt >= f1 else ''}{(f1_opt - f1) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  AUC-ROC   : {auc:.3f} ‚Üí {auc_opt:.3f} ({'+' if auc_opt >= auc else ''}{(auc_opt - auc) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2.8 Visualisation Finale : Matrice de Confusion et Feature Importance\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Matrice de confusion du mod√®le optimis√©\n",
    "ax1 = axes[0]\n",
    "cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_opt, display_labels=[\"Non-Dropout\", \"Dropout\"])\n",
    "disp.plot(ax=ax1, cmap=\"Blues\", values_format=\"d\")\n",
    "ax1.set_title(\n",
    "    \"Matrice de Confusion\\n(Logistic Regression Optimis√©)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# 2. Feature Importance (coefficients absolus)\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Extraire les coefficients du mod√®le\n",
    "classifier = best_model.named_steps[\"classifier\"]\n",
    "coefficients = classifier.coef_[0]\n",
    "\n",
    "# R√©cup√©rer les noms des features apr√®s transformation\n",
    "feature_names = (\n",
    "    numeric_features\n",
    "    + list(\n",
    "        best_model.named_steps[\"preprocessor\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    "    + binary_features\n",
    ")\n",
    "\n",
    "# Cr√©er un DataFrame pour visualisation\n",
    "coef_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": feature_names,\n",
    "        \"Coefficient\": coefficients,\n",
    "        \"Abs_Coefficient\": np.abs(coefficients),\n",
    "    }\n",
    ").sort_values(\"Abs_Coefficient\", ascending=True)\n",
    "\n",
    "# Top 10 features les plus importantes\n",
    "top_features = coef_df.tail(10)\n",
    "colors = [\"#e74c3c\" if c < 0 else \"#2ecc71\" for c in top_features[\"Coefficient\"]]\n",
    "ax2.barh(top_features[\"Feature\"], top_features[\"Coefficient\"], color=colors)\n",
    "ax2.axvline(x=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax2.set_xlabel(\"Coefficient (impact sur le dropout)\", fontsize=11)\n",
    "ax2.set_title(\n",
    "    \"Top 10 Features les Plus Influentes\\n(Rouge = ‚Üë risque, Vert = ‚Üì risque)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpr√©tation\n",
    "print(\"\\nüí° Interpr√©tation des Coefficients:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"  üî¥ Coefficient POSITIF ‚Üí Augmente le risque de dropout\")\n",
    "print(\"  üü¢ Coefficient N√âGATIF ‚Üí Diminue le risque de dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Synth√®se Phase 6.2 : Comparaison et Optimisation\n",
    "\n",
    "**R√©sultats de la comparaison (Cross-Validation 5-Fold)** :\n",
    "\n",
    "| Mod√®le | F1-Score | Recall | AUC-ROC |\n",
    "|--------|----------|--------|---------|\n",
    "| SVM (RBF) | 0.790 ¬±0.015 | 0.790 ¬±0.019 | 0.908 ¬±0.010 |\n",
    "| Logistic Regression | 0.784 ¬±0.021 | 0.799 ¬±0.024 | 0.913 ¬±0.012 |\n",
    "| Gradient Boosting | 0.783 ¬±0.013 | 0.726 ¬±0.012 | 0.911 ¬±0.013 |\n",
    "| Random Forest | 0.774 ¬±0.018 | 0.713 ¬±0.012 | 0.906 ¬±0.013 |\n",
    "\n",
    "**Choix et Optimisation** :\n",
    "- Mod√®le choisi : **Logistic Regression** (meilleur Recall, interpr√©table)\n",
    "- Hyperparam√®tres optimaux : `C=1`, `penalty='l1'`, `solver='saga'`\n",
    "\n",
    "**R√©sultats du mod√®le optimis√© (Test Set)** :\n",
    "\n",
    "| M√©trique | Baseline | Optimis√© | Changement |\n",
    "|----------|----------|----------|------------|\n",
    "| Recall (Dropout) | 83.1% | 83.1% | +0.0% |\n",
    "| F1-Score (Dropout) | 81.4% | 81.5% | +0.1% |\n",
    "| AUC-ROC | 93.1% | 93.1% | +0.0% |\n",
    "\n",
    "**Conclusion** :\n",
    "1. Les 4 mod√®les test√©s ont des performances tr√®s similaires\n",
    "2. L'optimisation des hyperparam√®tres n'a pas significativement am√©lior√© les r√©sultats\n",
    "3. Cela confirme que les **features engineered** sont le facteur cl√© de la performance\n",
    "4. Le mod√®le baseline √©tait d√©j√† proche de l'optimal gr√¢ce √† l'EDA de qualit√©\n",
    "\n",
    "**Top 5 Features les plus importantes** (Logistic Regression avec L1) :\n",
    "1. `Success_Rate_Sem2` - Taux de r√©ussite 2√®me semestre\n",
    "2. `Success_Rate_Sem1` - Taux de r√©ussite 1er semestre\n",
    "3. `Tuition fees up to date` - Frais de scolarit√© √† jour\n",
    "4. `Age_Group_36+` - √âtudiants de 36 ans et plus\n",
    "5. `Avg_Grade` - Moyenne des notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## Phase 6.3 : D√©ploiement du Mod√®le\n",
    "\n",
    "**Objectif** : Sauvegarder le mod√®le entra√Æn√© et cr√©er une interface de pr√©diction r√©utilisable.\n",
    "\n",
    "Cette phase comprend :\n",
    "1. Sauvegarde du pipeline complet (pr√©traitement + mod√®le)\n",
    "2. Fonction de pr√©diction pour de nouvelles donn√©es\n",
    "3. Documentation et exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.3.1 SAUVEGARDE DU MOD√àLE\n",
    "# =============================================================================\n",
    "\n",
    "# Cr√©er le r√©pertoire models/ s'il n'existe pas\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Nom du fichier avec version\n",
    "model_filename = \"../models/dropout_predictor_v1.joblib\"\n",
    "\n",
    "# Sauvegarder le pipeline complet (pr√©traitement + mod√®le)\n",
    "# best_model est cr√©√© dans la cellule d'√©valuation (grid_search.best_estimator_)\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# V√©rifier la taille du fichier\n",
    "file_size = os.path.getsize(model_filename) / 1024  # en KB\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELE SAUVEGARDE AVEC SUCCES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFichier: {model_filename}\")\n",
    "print(f\"Taille: {file_size:.1f} KB\")\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Informations sur le mod√®le sauvegard√©\n",
    "print(\"\\nContenu du pipeline:\")\n",
    "print(\"   1. ColumnTransformer (pr√©traitement)\")\n",
    "print(\"      - StandardScaler pour variables num√©riques\")\n",
    "print(\"      - OneHotEncoder pour variables cat√©gorielles\")\n",
    "print(\"      - Passthrough pour variables binaires\")\n",
    "print(\"   2. LogisticRegression (mod√®le optimis√©)\")\n",
    "print(f\"      - C={best_model.named_steps['classifier'].C}\")\n",
    "print(f\"      - penalty={best_model.named_steps['classifier'].penalty}\")\n",
    "print(f\"      - class_weight={best_model.named_steps['classifier'].class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.3.2 FONCTION DE PR√âDICTION R√âUTILISABLE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def predict_dropout(student_data, model_path=\"../models/dropout_predictor_v1.joblib\"):\n",
    "    \"\"\"\n",
    "    Pr√©dit le risque de dropout pour un √©tudiant.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_data : dict ou pd.DataFrame\n",
    "        Donn√©es de l'√©tudiant avec les features requises\n",
    "    model_path : str\n",
    "        Chemin vers le mod√®le sauvegard√©\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Pr√©diction (0/1), probabilit√© de dropout, et niveau de risque\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    # Charger le mod√®le\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # Convertir en DataFrame si n√©cessaire\n",
    "    if isinstance(student_data, dict):\n",
    "        student_df = pd.DataFrame([student_data])\n",
    "    else:\n",
    "        student_df = student_data.copy()\n",
    "\n",
    "    # Pr√©dire\n",
    "    prediction = model.predict(student_df)[0]\n",
    "    proba = model.predict_proba(student_df)[0, 1]\n",
    "\n",
    "    # D√©terminer le niveau de risque\n",
    "    if proba < 0.3:\n",
    "        risk_level = \"üü¢ FAIBLE\"\n",
    "    elif proba < 0.5:\n",
    "        risk_level = \"üü° MOD√âR√â\"\n",
    "    elif proba < 0.7:\n",
    "        risk_level = \"üü† √âLEV√â\"\n",
    "    else:\n",
    "        risk_level = \"üî¥ CRITIQUE\"\n",
    "\n",
    "    return {\n",
    "        \"prediction\": \"Dropout\" if prediction == 1 else \"Non-Dropout\",\n",
    "        \"probability\": round(proba, 3),\n",
    "        \"risk_level\": risk_level,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test avec un exemple du dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TEST DE LA FONCTION DE PR√âDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S√©lectionner un √©tudiant al√©atoire du test set\n",
    "test_sample = X_test.iloc[0:1]  # Premier √©tudiant du test set\n",
    "actual_label = y_test.iloc[0]\n",
    "\n",
    "print(\"\\nüìã Donn√©es de l'√©tudiant test:\")\n",
    "print(test_sample.to_string())\n",
    "\n",
    "result = predict_dropout(test_sample)\n",
    "\n",
    "print(\"\\nüéØ R√©sultat de la pr√©diction:\")\n",
    "print(f\"   Pr√©diction     : {result['prediction']}\")\n",
    "print(f\"   Probabilit√©    : {result['probability']:.1%}\")\n",
    "print(f\"   Niveau de risque: {result['risk_level']}\")\n",
    "print(f\"   Valeur r√©elle  : {'Dropout' if actual_label == 1 else 'Non-Dropout'}\")\n",
    "print(\n",
    "    f\"   Correct        : {'‚úÖ' if (result['prediction'] == 'Dropout') == (actual_label == 1) else '‚ùå'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Documentation - Utilisation du Mod√®le\n",
    "\n",
    "#### Features Requises\n",
    "\n",
    "Le mod√®le attend un DataFrame avec les 16 colonnes suivantes :\n",
    "\n",
    "| Feature | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| `Success_Rate_Sem1` | float | Ratio unit√©s valid√©es/inscrites au 1er semestre (0-1) |\n",
    "| `Success_Rate_Sem2` | float | Ratio unit√©s valid√©es/inscrites au 2√®me semestre (0-1) |\n",
    "| `Avg_Grade` | float | Moyenne des notes sur les 2 semestres (0-20) |\n",
    "| `Total_Approved` | int | Total des unit√©s valid√©es (sem1 + sem2) |\n",
    "| `Age at enrollment` | int | √Çge √† l'inscription |\n",
    "| `Admission grade` | float | Note d'admission (0-200) |\n",
    "| `Performance_Trend` | float | Diff√©rence grade sem2 - grade sem1 |\n",
    "| `Age_Group` | str | '17-20', '21-25', '26-35', ou '36+' |\n",
    "| `Course_Domain` | str | 'Tech', 'Sant√©', 'Business', 'Social', 'Arts', 'Education' |\n",
    "| `Marital_Binary` | str | 'Solo' ou 'Couple' |\n",
    "| `Education_Level` | str | 'Secondaire', 'Sup√©rieur', ou 'Autre' |\n",
    "| `Tuition fees up to date` | int | 1 si frais √† jour, 0 sinon |\n",
    "| `Scholarship holder` | int | 1 si boursier, 0 sinon |\n",
    "| `Debtor` | int | 1 si d√©biteur, 0 sinon |\n",
    "| `Gender` | int | 1 = Homme, 0 = Femme |\n",
    "| `Displaced` | int | 1 si d√©plac√©, 0 sinon |\n",
    "\n",
    "#### Exemple d'utilisation\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le mod√®le\n",
    "model = joblib.load('models/dropout_predictor_v1.joblib')\n",
    "\n",
    "# Cr√©er les donn√©es d'un √©tudiant\n",
    "student = pd.DataFrame([{\n",
    "    'Success_Rate_Sem1': 0.8,\n",
    "    'Success_Rate_Sem2': 0.7,\n",
    "    'Avg_Grade': 12.5,\n",
    "    'Total_Approved': 10,\n",
    "    'Age at enrollment': 20,\n",
    "    'Admission grade': 140.0,\n",
    "    'Performance_Trend': -1.0,\n",
    "    'Age_Group': '17-20',\n",
    "    'Course_Domain': 'Tech',\n",
    "    'Marital_Binary': 'Solo',\n",
    "    'Education_Level': 'Secondaire',\n",
    "    'Tuition fees up to date': 1,\n",
    "    'Scholarship holder': 0,\n",
    "    'Debtor': 0,\n",
    "    'Gender': 1,\n",
    "    'Displaced': 0\n",
    "}])\n",
    "\n",
    "# Pr√©dire\n",
    "prediction = model.predict(student)[0]\n",
    "probability = model.predict_proba(student)[0, 1]\n",
    "\n",
    "print(f\"Pr√©diction: {'Dropout' if prediction == 1 else 'Non-Dropout'}\")\n",
    "print(f\"Probabilit√© de dropout: {probability:.1%}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Synth√®se Phase 6.3 : D√©ploiement du Mod√®le\n",
    "\n",
    "#### R√©alisations\n",
    "\n",
    "1. **Mod√®le Sauvegard√©** : `models/dropout_predictor_v1.joblib` (5.7 KB)\n",
    "   - Pipeline complet incluant pr√©traitement + mod√®le\n",
    "   - Portable et r√©utilisable sans d√©pendance au notebook\n",
    "\n",
    "2. **Fonction de Pr√©diction** : `predict_dropout()`\n",
    "   - Accepte dict ou DataFrame\n",
    "   - Retourne pr√©diction, probabilit√© et niveau de risque\n",
    "   - Cat√©gorisation du risque en 4 niveaux (Faible/Mod√©r√©/√âlev√©/Critique)\n",
    "\n",
    "3. **Documentation** : Guide complet des features requises\n",
    "\n",
    "#### Performance du Mod√®le D√©ploy√©\n",
    "\n",
    "| M√©trique | Valeur |\n",
    "|----------|--------|\n",
    "| **Recall (Dropout)** | 83.1% |\n",
    "| **F1-Score** | 81.5% |\n",
    "| **AUC-ROC** | 93.1% |\n",
    "| **Pr√©cision globale** | 88% |\n",
    "\n",
    "#### Top 3 Pr√©dicteurs\n",
    "\n",
    "1. `Success_Rate_Sem2` - Taux de r√©ussite 2√®me semestre\n",
    "2. `Success_Rate_Sem1` - Taux de r√©ussite 1er semestre  \n",
    "3. `Tuition fees up to date` - Frais de scolarit√© √† jour\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion de l'Analyse\n",
    "\n",
    "Ce projet a d√©montr√© une approche compl√®te de machine learning pour la pr√©diction du dropout √©tudiant :\n",
    "\n",
    "1. **EDA approfondie** - Exploration des 37 variables, identification des patterns\n",
    "2. **Feature Engineering** - Cr√©ation de 9 nouvelles features pertinentes\n",
    "3. **Mod√©lisation** - Comparaison de 4 algorithmes, optimisation des hyperparam√®tres\n",
    "4. **D√©ploiement** - Mod√®le portable et fonction de pr√©diction r√©utilisable\n",
    "\n",
    "**Insight cl√©** : La qualit√© du feature engineering (ratios de r√©ussite) a eu plus d'impact que le choix de l'algorithme. Les variables acad√©miques du 2√®me semestre sont les meilleurs pr√©dicteurs du dropout."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
